{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web_scraper.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "--UEDSjhPb5v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "2a1e2169-738f-45c9-cfc2-5d16c5b14768"
      },
      "cell_type": "code",
      "source": [
        "!pip install stop-words"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stop-words\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3Rf8S9QtPgQE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import json\n",
        "import operator\n",
        "from tabulate import tabulate\n",
        "import sys\n",
        "from stop_words import get_stop_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MjMB3G8Lbhm_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get the words\n",
        "def getWordList(url):\n",
        "  word_list = []\n",
        "  \n",
        "  #raw_data\n",
        "  source_code = requests.get(url)\n",
        "  \n",
        "  #convert to text\n",
        "  plain_text = source_code.text\n",
        "  \n",
        "  #lxml format\n",
        "  soup = BeautifulSoup(plain_text,'lxml')\n",
        "  \n",
        "  #findthe words in paragraph tag\n",
        "  \n",
        "  for text in soup.findAll('p'):\n",
        "    if text.text is None:\n",
        "      continue\n",
        "    content = text.text\n",
        "    words = content.lower().split()\n",
        "    \n",
        "    for word in words:\n",
        "      #remove non-chars\n",
        "      cleaned_word = clean_word(word)\n",
        "      #if there is something still there\n",
        "      if len(cleaned_word)>0:\n",
        "        #add it to our word_list\n",
        "        word_list.append(cleaned_word)\n",
        "  return word_list\n",
        "def createFrequecyTable(word_list):\n",
        "  word_count = {}\n",
        "  for word in word_list:\n",
        "    if word in word_count:\n",
        "      word_count[word] += 1\n",
        "    else:\n",
        "      word_count[word] = 1\n",
        "  return word_count\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fbDs3W3jO4zE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#clean words with regex\n",
        "def clean_word(word):\n",
        "  cleaned_word = re.sub('[^A-Za-z]+','',word)\n",
        "  return cleaned_word\n",
        "#remove stopwords\n",
        "def remove_stop_words(frequency_list):\n",
        "  stop_words = get_stop_words('en')\n",
        "  temp_list = []\n",
        "  for key,value in frequency_list:\n",
        "    if key not in stop_words:\n",
        "      temp_list.append([key,value])\n",
        "      \n",
        "  return temp_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v8mzRtB3QI8y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#keyword you want to search\n",
        "string_query = 'ai'\n",
        "\n",
        "#to remove stop words or not\n",
        "search_mode = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XUK7gD4FQbmG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wikipedia_api_link = 'https://en.wikipedia.org/w/api.php?format=json&action=query&list=search&srsearch='\n",
        "wikipedia_link = 'https://en.wikipedia.org/wiki/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "29NhiCzJQr1A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "fef35f8e-fa5d-41d5-8760-29326d59c33d"
      },
      "cell_type": "code",
      "source": [
        "url = wikipedia_api_link + string_query\n",
        "\n",
        "try:\n",
        "  #retreiving raw data from wiki api\n",
        "  response = requests.get(url)\n",
        "  \n",
        "  #formating data as json dictionary\n",
        "  data = json.loads(response.content.decode('utf-8'))\n",
        "  \n",
        "  #page title, first option\n",
        "  #show this in web browser\n",
        "  wikipedia_page_tag = data['query']['search'][0]['title']\n",
        "  \n",
        "  #get actual wiki page based on retrieved title\n",
        "  url = wikipedia_link + wikipedia_page_tag\n",
        "  \n",
        "  #get list of words from that page\n",
        "  page_word_list = getWordList(url)\n",
        "  \n",
        "  #create table of word counts, dictionary\n",
        "  page_word_count = createFrequecyTable(page_word_list)\n",
        "  \n",
        "  #sort the table by the frequency count\n",
        "  sorted_word_frequency_list = sorted(page_word_count.items(), key=operator.itemgetter(1), reverse=True)\n",
        "  \n",
        "  #remove stop words if the user specified\n",
        "  if(search_mode):\n",
        "    sorted_word_frequency_list = remove_stop_words(sorted_word_frequency_list)\n",
        "\n",
        "  #sum the total words to calculate frequencies   \n",
        "  total_words_sum = 0\n",
        "  for key,value in sorted_word_frequency_list:\n",
        "      total_words_sum = total_words_sum + value\n",
        "\n",
        "  #just get the top 20 words\n",
        "  if len(sorted_word_frequency_list) > 20:\n",
        "      sorted_word_frequency_list = sorted_word_frequency_list[:20]\n",
        "\n",
        "  #create our final list which contains words, frequency (word count), percentage\n",
        "  final_list = []\n",
        "  for key,value in sorted_word_frequency_list:\n",
        "      percentage_value = float(value * 100) / total_words_sum\n",
        "      final_list.append([key, value, round(percentage_value, 4)])\n",
        "\n",
        "  #headers before the table\n",
        "  print_headers = ['Word', 'Frequency', 'Frequency Percentage']\n",
        "\n",
        "  #print the table with tabulate\n",
        "  print(tabulate(final_list, headers=print_headers, tablefmt='orgtbl'))\n",
        "\n",
        "#throw an exception in case it breaks\n",
        "except requests.exceptions.Timeout:\n",
        "    print(\"The server didn't respond. Please, try again later.\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Word         |   Frequency |   Frequency Percentage |\n",
            "|--------------+-------------+------------------------|\n",
            "| ai           |         166 |                 2.1765 |\n",
            "| intelligence |          86 |                 1.1276 |\n",
            "| can          |          83 |                 1.0882 |\n",
            "| artificial   |          65 |                 0.8522 |\n",
            "| human        |          63 |                 0.826  |\n",
            "| learning     |          59 |                 0.7736 |\n",
            "| many         |          50 |                 0.6556 |\n",
            "| machine      |          47 |                 0.6162 |\n",
            "| research     |          44 |                 0.5769 |\n",
            "| networks     |          39 |                 0.5113 |\n",
            "| knowledge    |          37 |                 0.4851 |\n",
            "| neural       |          34 |                 0.4458 |\n",
            "| use          |          33 |                 0.4327 |\n",
            "| problems     |          32 |                 0.4196 |\n",
            "| also         |          31 |                 0.4065 |\n",
            "| computer     |          30 |                 0.3933 |\n",
            "| used         |          29 |                 0.3802 |\n",
            "| machines     |          28 |                 0.3671 |\n",
            "| systems      |          27 |                 0.354  |\n",
            "| problem      |          26 |                 0.3409 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fHt0lacOUYY-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}